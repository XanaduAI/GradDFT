{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Consistent Field Calculations with Neural Functionals\n",
    "\n",
    "In this tutorial, we will cover how to create to perform self consistent field (SCF)calculations with `NeuralFunctionals`. Many of the implementations of SCF methods in Grad DFT are fully differentiable so will come in useful when we wish to accurately train functionals. \n",
    "\n",
    "Others are not fully differentiable but are still useful when one need to converge a SCF loop in Grad DFT when other methods may fail. These methods bypass the SCF loop by directly minimizing the energy with respect to the Kohn-Sham orbital coefficients.\n",
    "\n",
    "To begin, we will run most of the code cells from the previous tutorial `~/examples/basic_notebooks/example_neural_functional_04.ipynb` such that we have a basic `NeuralFunctional` instance and a dummy \"truth energy\" from an LDA calculation in PySCF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablo.casares/miniforge3/envs/graddft/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting \"B3LYP_WITH_VWN5 = True\" in pyscf_conf.py\n",
      "  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/pablo.casares/miniforge3/envs/graddft/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "converged SCF energy = -1.11599939445016  <S^2> = 4.4408921e-16  2S+1 = 1\n"
     ]
    }
   ],
   "source": [
    "from pyscf import gto, dft\n",
    "import grad_dft as gd\n",
    " \n",
    "# Define the geometry of the molecule\n",
    "mol = gto.M(atom=[[\"H\", (0, 0, 0)], [\"H\", (0, 0, 1)]], basis=\"def2-tzvp\", charge=0, spin=0)\n",
    "mf = dft.UKS(mol)\n",
    "ground_truth_energy = mf.kernel()\n",
    "\n",
    "# Then we can use the following function to generate the molecule object\n",
    "HH_molecule = gd.molecule_from_pyscf(mf)\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def coefficient_inputs(molecule: gd.Molecule, *_, **__):\n",
    "    rho = molecule.density()\n",
    "    kinetic = molecule.kinetic_density()\n",
    "    return jnp.concatenate((rho, kinetic), axis = 1)\n",
    "\n",
    "def energy_densities(molecule: gd.Molecule, clip_cte: float = 1e-30, *_, **__):\n",
    "    r\"\"\"Auxiliary function to generate the features of LSDA.\"\"\"\n",
    "    # Molecule can compute the density matrix.\n",
    "    rho = jnp.clip(molecule.density(), a_min=clip_cte)\n",
    "    # Now we can implement the LDA energy density equation in the paper.\n",
    "    lda_e = -3/2 * (3/(4*jnp.pi)) ** (1/3) * (rho**(4/3)).sum(axis = 1, keepdims = True)\n",
    "    # For simplicity we do not include the exchange polarization correction\n",
    "    # check function exchange_polarization_correction in functional.py\n",
    "    # The output of features must be an Array of dimension n_grid x n_features.\n",
    "    return lda_e\n",
    "\n",
    "from flax import linen as nn\n",
    "from jax.nn import sigmoid\n",
    "\n",
    "out_features = 1\n",
    "def coefficients(instance, rhoinputs):\n",
    "    r\"\"\"\n",
    "    Instance is an instance of the class Functional or NeuralFunctional.\n",
    "    rhoinputs is the input to the neural network, in the form of an array.\n",
    "    localfeatures represents the potentials e_\\theta(r).\n",
    "\n",
    "    The output of this function is the energy density of the system.\n",
    "    \"\"\"\n",
    "\n",
    "    x = nn.Dense(features=out_features)(rhoinputs)\n",
    "    x = nn.LayerNorm()(x)\n",
    "    return sigmoid(x)\n",
    "\n",
    "nf = gd.NeuralFunctional(coefficients, energy_densities, coefficient_inputs)\n",
    "\n",
    "from jax.random import PRNGKey\n",
    "\n",
    "key = PRNGKey(42)\n",
    "cinputs = coefficient_inputs(HH_molecule)\n",
    "params = nf.init(key, cinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the total energy can be calculated in a non-self consistent way like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural functional non-SCF total energy with random parameters is -0.7769992\n"
     ]
    }
   ],
   "source": [
    "E_non_scf = nf.energy(params, HH_molecule)\n",
    "print(\"Neural functional non-SCF total energy with random parameters is\", E_non_scf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear mixing\n",
    "\n",
    "The most simple (but robust) way of performing a SCF calculation in DFT is [linear mixing of the density](http://www.numis.northwestern.edu/Presentations/DFT_Mixing_For_Dummies.pdf). This is implemented in `make_simple_scf_loop` in a robust (but non-JIT-compilable) format and in `diff_simple_scf_loop` in a JIT-compilable format. \n",
    "\n",
    "Let's make both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_mix = gd.simple_scf_loop(nf, mixing_factor=0.3, cycles=25)\n",
    "linear_mix_jit = gd.diff_simple_scf_loop(nf, mixing_factor=0.3, cycles=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and compute the SCF total energy for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablo.casares/Developer/GradDFT/grad_dft/molecule.py:312: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  nelecs = jnp.array([self.mo_occ[i].sum() for i in range(2)], dtype=jnp.int64)\n",
      "/Users/pablo.casares/miniforge3/envs/graddft/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:147: UserWarning: Explicitly requested dtype int64 requested in asarray is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return asarray(x, dtype=self.dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear mixing (non-JIT) total energy is -0.7908802\n",
      "Linear mixing (JIT) total energy is -0.7908811\n"
     ]
    }
   ],
   "source": [
    "mol_linear_mix = linear_mix(params, HH_molecule)\n",
    "mol_linear_mix_jit = linear_mix_jit(params, HH_molecule)\n",
    "\n",
    "print(\"Linear mixing (non-JIT) total energy is\", mol_linear_mix.energy)\n",
    "print(\"Linear mixing (JIT) total energy is\", mol_linear_mix_jit.energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try benchmarking the speed of both yourself. The Jitted version should be faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Inversion of the Iterative Subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Direct Inversion of the Iterative Subspace](https://en.wikipedia.org/wiki/DIIS) (DIIS) is a more complex method used in many code to quickly converge the SCF. Like linear mixing, we have a non-JIT and JIT version implemented. \n",
    "\n",
    "The default functions are created like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "diis = gd.scf_loop(nf, cycles=5)\n",
    "diis_jit = gd.diff_scf_loop(nf, cycles=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and are evaluated in the same way as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCF not converged.\n",
      "SCF energy = -0.918728046617933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablo.casares/Developer/GradDFT/grad_dft/molecule.py:312: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  nelecs = jnp.array([self.mo_occ[i].sum() for i in range(2)], dtype=jnp.int64)\n",
      "/Users/pablo.casares/Developer/GradDFT/grad_dft/molecule.py:312: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  nelecs = jnp.array([self.mo_occ[i].sum() for i in range(2)], dtype=jnp.int64)\n",
      "/Users/pablo.casares/Developer/GradDFT/grad_dft/molecule.py:312: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  nelecs = jnp.array([self.mo_occ[i].sum() for i in range(2)], dtype=jnp.int64)\n",
      "/Users/pablo.casares/Developer/GradDFT/grad_dft/molecule.py:312: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  nelecs = jnp.array([self.mo_occ[i].sum() for i in range(2)], dtype=jnp.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIIS (non-JIT) total energy is -0.79088175\n",
      "DIIS (JIT) total energy is -0.7908813\n"
     ]
    }
   ],
   "source": [
    "mol_diis = diis(params, HH_molecule)\n",
    "mol_diis_jit = diis_jit(params, HH_molecule)\n",
    "\n",
    "print(\"DIIS (non-JIT) total energy is\", mol_diis.energy)\n",
    "print(\"DIIS (JIT) total energy is\", mol_diis_jit.energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will remind you now that all of these SCF iterators so far, DIIS or linear mixing, are fully differentiable which means we have access to the gradients of any SCF predicted property (like the total energy, density etc.) with respect to the parameters of the neural functional. This means that neural functionals can be trained self consistently. We will encounter this in the `intermediate_notebooks`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Kohn-Sham orbitals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a further method for total energy calculation in Grad DFT. This involes a direct minimization of the total energy with respect to the Kohn-Sham orbital coefficients. This process is not presently differentiable but can come in useful in cases where the total energy is not converging in the SCF loops above. \n",
    "\n",
    "To use this method, we require an optimizer from `optax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optax import adam\n",
    "\n",
    "learning_rate = 1e-5\n",
    "momentum = 0.9\n",
    "tx = adam(learning_rate=learning_rate, b1=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make callable non-jittable and jittable versions of the orbital optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "orb_opt = gd.mol_orb_optimizer(nf, tx, cycles=20)\n",
    "orb_opt_jit = gd.jitted_mol_orb_optimizer(nf, tx, cycles=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and calculate the total energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orbital optimizer (non-JIT) total energy is -0.707909\n",
      "Orbital optimizer (JIT) total energy is -0.70790976\n"
     ]
    }
   ],
   "source": [
    "mol_orb_opt = orb_opt(params, HH_molecule)\n",
    "mol_orb_opt_jit = orb_opt_jit(params, HH_molecule)\n",
    "\n",
    "print(\"Orbital optimizer (non-JIT) total energy is\", mol_orb_opt.energy)\n",
    "print(\"Orbital optimizer (JIT) total energy is\", mol_orb_opt_jit.energy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad_dft_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
